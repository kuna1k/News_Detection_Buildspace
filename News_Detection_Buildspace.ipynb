{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cf7a86f-a6d2-480b-8565-0f2057373e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re  # For regular expressions, used in text preprocessing\n",
    "import nltk  # Natural Language Toolkit for text processing\n",
    "from nltk.corpus import stopwords  # Stopwords list from NLTK\n",
    "from nltk.stem.porter import PorterStemmer  # PorterStemmer for stemming words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF Vectorizer for text to numeric conversion\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # For dataset splitting and hyperparameter tuning\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Evaluation metrics\n",
    "import joblib  # For saving the trained model\n",
    "import logging  # For logging the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4611214-5296-4641-8776-5e2b99fe9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kunalk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-08-21 22:53:10,981 - INFO - Stopwords from NLTK: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Setting up logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Downloading necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Displaying the stopwords in English\n",
    "logging.info(f\"Stopwords from NLTK: {stopwords.words('english')}\")\n",
    "\n",
    "# Utility function for text preprocessing\n",
    "def preprocess_text(content):\n",
    "    \"\"\"\n",
    "    Clean, lower, split, and stem the input text using regular expressions, stopwords removal, and stemming.\n",
    "    \"\"\"\n",
    "    # Initialize the PorterStemmer\n",
    "    port_stem = PorterStemmer()\n",
    "    \n",
    "    # Remove all non-alphabetical characters\n",
    "    content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    # Convert to lowercase\n",
    "    content = content.lower()\n",
    "    # Split the text into individual words\n",
    "    words = content.split()\n",
    "    # Stem words and remove stopwords\n",
    "    words = [port_stem.stem(word) for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Rejoin words into a single string\n",
    "    processed_content = ' '.join(words)\n",
    "    \n",
    "    return processed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b888e3e0-eece-4e65-a067-b907e331123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 22:54:08,764 - INFO - Dataset loaded successfully. Shape: (20800, 5)\n",
      "2024-08-21 22:54:08,769 - INFO - Missing values in the dataset:\n",
      "id           0\n",
      "title      558\n",
      "author    1957\n",
      "text        39\n",
      "label        0\n",
      "dtype: int64\n",
      "2024-08-21 22:54:08,773 - INFO - Missing values have been replaced with empty strings.\n",
      "2024-08-21 22:54:17,794 - INFO - Preprocessed 'content' column:\n",
      "0    darrel lucu hous dem aid even see comey letter...\n",
      "1    daniel j flynn flynn hillari clinton big woman...\n",
      "2               consortiumnew com truth might get fire\n",
      "3    jessica purkiss civilian kill singl us airstri...\n",
      "4    howard portnoy iranian woman jail fiction unpu...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset into a pandas DataFrame\n",
    "file_path = 'train.csv'  # Path to the dataset\n",
    "try:\n",
    "    news_dataset = pd.read_csv(file_path)\n",
    "    logging.info(f\"Dataset loaded successfully. Shape: {news_dataset.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"File not found. Please check the path: {file_path}\")\n",
    "    raise\n",
    "\n",
    "# Checking for missing values in the dataset\n",
    "missing_values = news_dataset.isnull().sum()\n",
    "logging.info(f\"Missing values in the dataset:\\n{missing_values}\")\n",
    "\n",
    "# Replacing missing values with empty strings to avoid issues during text processing\n",
    "news_dataset.fillna('', inplace=True)\n",
    "logging.info(\"Missing values have been replaced with empty strings.\")\n",
    "\n",
    "# Merging the 'author' and 'title' columns to create a new 'content' column\n",
    "news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']\n",
    "\n",
    "# Applying text preprocessing to the 'content' column\n",
    "news_dataset['content'] = news_dataset['content'].apply(preprocess_text)\n",
    "\n",
    "# Separating the features (X) and the target label (Y)\n",
    "X = news_dataset['content'].values\n",
    "Y = news_dataset['label'].values\n",
    "\n",
    "# Displaying the first few rows of preprocessed data\n",
    "logging.info(f\"Preprocessed 'content' column:\\n{news_dataset['content'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "653ebf84-b3e4-4259-b5d2-c725a850ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 22:54:35,687 - INFO - Shape of the transformed data (X): (20800, 5000)\n",
      "2024-08-21 22:54:35,689 - INFO - First 10 feature names generated by the vectorizer:\n",
      "['aaron' 'aaron carrol' 'aaron kesel' 'aaron klein' 'abandon' 'abbi'\n",
      " 'abbi goodnough' 'abc' 'abduct' 'abe']\n",
      "2024-08-21 22:54:35,889 - INFO - Training set shape: X_train: (16640, 5000), Y_train: (16640,)\n",
      "2024-08-21 22:54:35,889 - INFO - Test set shape: X_test: (4160, 5000), Y_test: (4160,)\n"
     ]
    }
   ],
   "source": [
    "# Converting textual data to numerical data using TF-IDF Vectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer with optimized parameters\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer on the text data and transform the content into vectors\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Displaying the shape of the transformed data to verify the conversion\n",
    "logging.info(f\"Shape of the transformed data (X): {X.shape}\")\n",
    "\n",
    "# Display the first few feature names generated by the vectorizer for inspection\n",
    "logging.info(f\"First 10 feature names generated by the vectorizer:\\n{vectorizer.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# Splitting the dataset into training and test sets using stratified sampling\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "# Displaying the shape of the training and test sets\n",
    "logging.info(f\"Training set shape: X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "logging.info(f\"Test set shape: X_test: {X_test.shape}, Y_test: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4926a088-b120-457e-b778-ff0a01678fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 22:54:58,294 - INFO - Best model parameters: {'C': 10, 'max_iter': 100, 'solver': 'saga'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=10, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=10, solver='saga')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Setting up hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# Using GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Extracting the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "logging.info(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Training the best model on the entire training data\n",
    "best_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "864f8117-09e0-4b39-b48c-85c11c971c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 22:55:17,023 - INFO - Accuracy score on the training data: 0.9987\n",
      "2024-08-21 22:55:17,026 - INFO - Accuracy score on the test data: 0.9925\n",
      "2024-08-21 22:55:17,034 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.99      0.99      0.99      2077\n",
      "        Fake       0.99      0.99      0.99      2083\n",
      "\n",
      "    accuracy                           0.99      4160\n",
      "   macro avg       0.99      0.99      0.99      4160\n",
      "weighted avg       0.99      0.99      0.99      4160\n",
      "\n",
      "2024-08-21 22:55:17,036 - INFO - Confusion Matrix:\n",
      "[[2057   20]\n",
      " [  11 2072]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model's performance on the training data\n",
    "\n",
    "# Predicting the labels for the training set\n",
    "X_train_prediction = best_model.predict(X_train)\n",
    "\n",
    "# Calculating the accuracy score on the training data\n",
    "training_data_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
    "logging.info(f\"Accuracy score on the training data: {training_data_accuracy:.4f}\")\n",
    "\n",
    "# Evaluating the model's performance on the test data\n",
    "\n",
    "# Predicting the labels for the test set\n",
    "X_test_prediction = best_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy score on the test data\n",
    "test_data_accuracy = accuracy_score(Y_test, X_test_prediction)\n",
    "logging.info(f\"Accuracy score on the test data: {test_data_accuracy:.4f}\")\n",
    "\n",
    "# Generating a classification report for detailed performance metrics\n",
    "classification_report_str = classification_report(Y_test, X_test_prediction, target_names=['Real', 'Fake'])\n",
    "logging.info(f\"Classification Report:\\n{classification_report_str}\")\n",
    "\n",
    "# Generating a confusion matrix to see the distribution of predictions\n",
    "conf_matrix = confusion_matrix(Y_test, X_test_prediction)\n",
    "logging.info(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fee44e0c-e45e-4992-bdeb-46f3006387ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 22:55:42,393 - INFO - Model saved to best_logistic_regression_model.pkl\n",
      "2024-08-21 22:55:42,553 - INFO - TF-IDF vectorizer saved to tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Saving the trained model to a file\n",
    "model_filename = 'best_logistic_regression_model.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "logging.info(f\"Model saved to {model_filename}\")\n",
    "\n",
    "# Saving the vectorizer to a file\n",
    "vectorizer_filename = 'tfidf_vectorizer.pkl'\n",
    "joblib.dump(vectorizer, vectorizer_filename)\n",
    "logging.info(f\"TF-IDF vectorizer saved to {vectorizer_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b62b2a54-6260-4f0d-88c8-02a0dde8c21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 22:55:54,724 - INFO - Prediction for the sample news: Fake\n"
     ]
    }
   ],
   "source": [
    "# Loading the model and vectorizer\n",
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_vectorizer = joblib.load(vectorizer_filename)\n",
    "\n",
    "# Function to predict whether news is real or fake\n",
    "def predict_news(news_text):\n",
    "    \"\"\"\n",
    "    Predict whether a piece of news is real or fake.\n",
    "    :param news_text: str, the news content\n",
    "    :return: str, 'Real' or 'Fake'\n",
    "    \"\"\"\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(news_text)\n",
    "    # Vectorize the processed text\n",
    "    vectorized_text = loaded_vectorizer.transform([processed_text])\n",
    "    # Predict the label\n",
    "    prediction = loaded_model.predict(vectorized_text)\n",
    "    \n",
    "    return 'Real' if prediction[0] == 0 else 'Fake'\n",
    "\n",
    "# Example usage\n",
    "sample_news = \"The President announces new policies to boost economy.\"\n",
    "result = predict_news(sample_news)\n",
    "logging.info(f\"Prediction for the sample news: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fd33ff0-3b32-44ce-8780-39ad3166cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the fake news: Real\n"
     ]
    }
   ],
   "source": [
    "# Example Real News Article\n",
    "real_news = \"\"\"The latest report from the United Nations Intergovernmental Panel on Climate Change (IPCC) warns that global warming is accelerating at an unprecedented rate. The report, which is based on extensive scientific research, indicates that the world is likely to exceed the 1.5°C warming threshold within the next two decades if urgent measures are not taken.\n",
    "\n",
    "The findings underscore the need for immediate and ambitious actions to reduce greenhouse gas emissions. According to the report, the consequences of inaction could be catastrophic, leading to more frequent and severe weather events, rising sea levels, and disruptions to ecosystems and food supplies. The report calls on governments worldwide to strengthen their commitments under the Paris Agreement and to invest in sustainable energy solutions.\n",
    "\n",
    "The UN Secretary-General, António Guterres, described the report as \"a code red for humanity,\" urging world leaders to act swiftly to avert the worst impacts of climate change.\"\"\"\n",
    "\n",
    "# Use the predict_news function from your model to test this article\n",
    "result = predict_news(real_news)\n",
    "\n",
    "# Print the prediction result\n",
    "print(f\"Prediction for the fake news: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be448ec7-4857-4338-b7cd-d0f5abc753f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the fake news: Fake\n"
     ]
    }
   ],
   "source": [
    "# Example usage to test a fake news article\n",
    "fake_news = \"Scientists Discover a Cure for All Cancers in Pineapple Juice. A group of scientists has claimed that drinking pineapple juice every day can cure all types of cancers. They say that the natural enzymes found in the fruit can completely eliminate cancer cells in just weeks.\"\n",
    "result = predict_news(fake_news)\n",
    "print(f\"Prediction for the fake news: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
